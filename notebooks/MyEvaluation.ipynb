{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will evaluate some topic modeling techniques\n",
    "- LDA\n",
    "- NMF \n",
    "- Top2Vec\n",
    "- Bertopic \n",
    "    - bert classic (miniml-6)\n",
    "    - openai\n",
    "    - tweet-classification\n",
    "    - climabert\n",
    "    - universal sentence encoder\n",
    "\n",
    "\n",
    "for each of this i made different test with different parameters and different datasets changing the numebr of topics\n",
    "\n",
    "Dataset:\n",
    "- climate: 1669 preprocessed tweets \n",
    "- todo other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import Trainer, DataLoader\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading climate data\n",
      "created vocab\n",
      "5225\n",
      "words filtering done\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=\"climate\").prepare_docs(save=\"climate.txt\").preprocess_octis(output_folder=\"climate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "============\n",
      "npmi: -0.05603186703787161\n",
      "umass: -4.021981614659724\n",
      "diversity: 0.47\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.04093330696194557\n",
      "umass: -5.894696977162936\n",
      "diversity: 0.475\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.04475179399892415\n",
      "umass: -5.726567227973454\n",
      "diversity: 0.41\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.07069132820451146\n",
      "umass: -6.639760026513971\n",
      "diversity: 0.3675\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.05190434104957615\n",
      "umass: -6.7300911945612505\n",
      "diversity: 0.392\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.05631840316969816\n",
      "umass: -3.7619125029036433\n",
      "diversity: 0.5\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.0684209728954257\n",
      "umass: -5.738021586252655\n",
      "diversity: 0.43\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.05425098973475934\n",
      "umass: -5.751620654004211\n",
      "diversity: 0.37333333333333335\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.05317002138686715\n",
      "umass: -6.506559030731209\n",
      "diversity: 0.3875\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.07643445436005926\n",
      "umass: -7.047234841772287\n",
      "diversity: 0.382\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.0309767664156465\n",
      "umass: -3.674720636876528\n",
      "diversity: 0.5\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.07403645270712103\n",
      "umass: -5.4273788221816766\n",
      "diversity: 0.405\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.06447445313296334\n",
      "umass: -6.395539544940193\n",
      "diversity: 0.39\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.055027317482541485\n",
      "umass: -6.411087558674756\n",
      "diversity: 0.415\n",
      " \n",
      "Results\n",
      "============\n",
      "npmi: -0.06521831805369326\n",
      "umass: -7.107350286896531\n",
      "diversity: 0.386\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset = \"climate\"\n",
    "    custom = True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "                      \n",
    "    results = trainer.train(save=f\"NMF_climate_{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_topics': [10, 20, 30, 40, 50], 'random_state': 42}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"climate\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"LDA__{i+1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bertopic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data preparation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we prepare the data for the bertopic model, so we get back the data as a list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Prepare data\n",
    "dataset, custom = \"climate\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a sentence_transformers model name or directly embeddings to evaluate \n",
    "def get_bertopic_result(model_name, embeddings = None, custom = False):\n",
    "\n",
    "        # get emebddings if not provided\n",
    "        display_name = model_name\n",
    "        model_name = None\n",
    "        results = []\n",
    "\n",
    "        if not custom:\n",
    "                model = SentenceTransformer(display_name)\n",
    "                embeddings = model.encode(data, show_progress_bar=True)\n",
    "                print('embedded')\n",
    "                model_name = display_name\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "          # do this 3 times\n",
    "        for i in range(3):\n",
    "                # params that will be passed to Bertopic, model name none for custom embeddings\n",
    "                params = {\n",
    "                        \"embedding_model\": model_name,\n",
    "                        \"nr_topics\": [(i+1)*10 for i in range(5)],  # 10, 20, 30, 40, 50 topics\n",
    "                        \"min_topic_size\": [5,15],                   # 5, 15 documents per topic\n",
    "                        \"verbose\": True\n",
    "                }\n",
    "                # train\n",
    "                trainer = Trainer(      dataset=dataset,\n",
    "                                        model_name=\"BERTopic\",\n",
    "                                        params=params,\n",
    "                                        bt_embeddings=embeddings,\n",
    "                                        custom_dataset=True,\n",
    "                                        verbose=False)\n",
    "\n",
    "                results.append(trainer.train())\n",
    "                print(f\"Done with {display_name} {i+1}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# given the results \n",
    "def clean_results(results):\n",
    "        # create result df \n",
    "        result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "        # dataset used and model \n",
    "        dataset = results[0][0]['Dataset']\n",
    "        model = results[0][0]['Params']['embedding_model']\n",
    "\n",
    "        # fill the dataframe \n",
    "        for result in results:\n",
    "                for test in result:\n",
    "                        pd.Series([test['Dataset'], test['Params']['embedding_model'], test['Params']['nr_topics'], test['Params']['min_topic_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "                        result_df = result_df.append(pd.Series([test['Dataset'], test['Params']['embedding_model'], test['Params']['nr_topics'], test['Params']['min_topic_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns), ignore_index=True)\n",
    "\n",
    "        # groupby and get the mean for the 3 tests \n",
    "        result_df.set_index(['Dataset', 'model', 'nr_topics', 'min_topic_size'], inplace=True)\n",
    "        a = result_df.groupby(['nr_topics', 'min_topic_size']).mean()\n",
    "        a.reset_index(inplace=True)\n",
    "\n",
    "        #save the results\n",
    "        a['dataset'] = dataset\n",
    "        a['model'] = model\n",
    "        a[['dataset', 'model','nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity',\n",
    "                'computation_time']].to_csv('bertopic'+model+'.csv', index=False)\n",
    "                \n",
    "        return result_df\n",
    "\n",
    "def get_openai_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "        texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "\n",
    "        embs = openai.Embedding.create(input = texts, model=model)['data']\n",
    "        return np.array([np.array(emb['embedding']) for emb in embs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## climatebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0531fd9792c74fe3ac28a5588e303a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:16:38,880 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:16:39,109 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:16:56,910 - BERTopic - Reduced number of topics from 66 to 10\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:17:48,624 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:17:49,153 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:17:53,340 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:18:35,499 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:18:35,872 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:18:57,089 - BERTopic - Reduced number of topics from 63 to 20\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:19:38,914 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:19:39,242 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:19:41,497 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:20:20,028 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:20:20,324 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:20:46,049 - BERTopic - Reduced number of topics from 61 to 30\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:21:27,920 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:21:28,237 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:21:30,075 - BERTopic - Reduced number of topics from 3 to 3\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:22:08,499 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:22:08,838 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:22:41,519 - BERTopic - Reduced number of topics from 67 to 40\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:23:37,509 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:23:37,856 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:23:39,938 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:24:27,263 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:24:27,943 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:24:55,152 - BERTopic - Reduced number of topics from 67 to 50\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:25:37,429 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:25:37,766 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:25:39,753 - BERTopic - Reduced number of topics from 4 to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with climatebert/distilroberta-base-climate-f 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:26:18,704 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:26:19,030 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:26:37,899 - BERTopic - Reduced number of topics from 67 to 10\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:27:16,880 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:27:17,109 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:27:18,881 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:28:06,927 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:28:07,272 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:28:28,855 - BERTopic - Reduced number of topics from 74 to 20\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:29:04,706 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:29:04,942 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:29:06,692 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:29:47,215 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:29:47,654 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:30:09,873 - BERTopic - Reduced number of topics from 73 to 30\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:30:48,052 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:30:48,236 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:30:49,907 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:31:28,554 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:31:28,873 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:31:49,125 - BERTopic - Reduced number of topics from 52 to 40\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:32:34,912 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:32:35,210 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:32:37,229 - BERTopic - Reduced number of topics from 3 to 3\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:33:20,716 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:33:21,128 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:33:45,735 - BERTopic - Reduced number of topics from 68 to 50\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:34:33,620 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:34:34,012 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:34:36,354 - BERTopic - Reduced number of topics from 4 to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with climatebert/distilroberta-base-climate-f 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:35:14,591 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:35:14,895 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:35:32,416 - BERTopic - Reduced number of topics from 68 to 10\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:36:15,061 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:36:15,474 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:36:17,267 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:36:57,548 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:36:57,892 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:37:21,660 - BERTopic - Reduced number of topics from 69 to 20\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:38:06,174 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:38:06,462 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:38:08,376 - BERTopic - Reduced number of topics from 4 to 4\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /Users/alessiogandelli/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-16 22:38:58,475 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:38:58,774 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "climatebert_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "climatebert_results = get_bertopic_result(climatebert_name)\n",
    "clean_results(climatebert_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tweetclassifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a359aeaf6e432186dfd244102977fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:18:18,285 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:18:18,835 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:18:50,610 - BERTopic - Reduced number of topics from 76 to 10\n",
      "2023-03-16 21:19:31,827 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:19:32,025 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:19:33,003 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:20:11,932 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:20:12,230 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:20:39,171 - BERTopic - Reduced number of topics from 65 to 20\n",
      "2023-03-16 21:21:18,092 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:21:18,441 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:21:19,654 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:21:58,640 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:21:58,919 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:23:10,771 - BERTopic - Reduced number of topics from 75 to 30\n",
      "2023-03-16 21:23:59,156 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:23:59,572 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:24:04,360 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:25:11,320 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:25:12,875 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:27:04,495 - BERTopic - Reduced number of topics from 73 to 40\n",
      "2023-03-16 21:28:32,898 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:28:33,460 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:28:36,574 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:29:21,293 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:29:22,142 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:30:17,831 - BERTopic - Reduced number of topics from 66 to 50\n",
      "2023-03-16 21:31:16,922 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:31:17,345 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:31:19,041 - BERTopic - Reduced number of topics from 2 to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with louisbetsch/tweetclassification-bf-model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:32:09,793 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:32:10,103 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:32:43,745 - BERTopic - Reduced number of topics from 72 to 10\n",
      "2023-03-16 21:33:42,514 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:33:43,084 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:33:45,684 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:34:48,643 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:34:49,583 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:35:40,818 - BERTopic - Reduced number of topics from 69 to 20\n",
      "2023-03-16 21:37:03,496 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:37:04,375 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:37:10,000 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:38:30,735 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:38:31,246 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:39:28,145 - BERTopic - Reduced number of topics from 72 to 30\n",
      "2023-03-16 21:40:33,611 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:40:34,202 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:40:36,644 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:41:51,736 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:41:52,122 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:43:14,597 - BERTopic - Reduced number of topics from 64 to 40\n",
      "2023-03-16 21:44:40,400 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:44:40,910 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:44:46,364 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:45:54,873 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:45:55,315 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:47:14,026 - BERTopic - Reduced number of topics from 75 to 50\n",
      "2023-03-16 21:48:44,524 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:48:45,217 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:48:51,269 - BERTopic - Reduced number of topics from 2 to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with louisbetsch/tweetclassification-bf-model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:50:19,270 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:50:20,124 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:51:08,993 - BERTopic - Reduced number of topics from 67 to 10\n",
      "2023-03-16 21:52:37,054 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:52:37,518 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:52:41,048 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:54:03,326 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:54:04,107 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:54:59,335 - BERTopic - Reduced number of topics from 74 to 20\n",
      "2023-03-16 21:56:07,246 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:56:07,510 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:56:09,844 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 21:57:15,014 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:57:15,649 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:58:41,295 - BERTopic - Reduced number of topics from 73 to 30\n",
      "2023-03-16 21:59:51,380 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 21:59:51,954 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 21:59:53,592 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 22:01:03,408 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:01:04,158 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:01:58,465 - BERTopic - Reduced number of topics from 65 to 40\n",
      "2023-03-16 22:03:18,599 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:03:19,210 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:03:22,844 - BERTopic - Reduced number of topics from 2 to 2\n",
      "2023-03-16 22:04:49,332 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:04:50,359 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:06:44,576 - BERTopic - Reduced number of topics from 73 to 50\n",
      "2023-03-16 22:07:59,843 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 22:08:00,591 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 22:08:04,391 - BERTopic - Reduced number of topics from 2 to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with louisbetsch/tweetclassification-bf-model 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>npmi</th>\n",
       "      <th>umass</th>\n",
       "      <th>diversity</th>\n",
       "      <th>computation_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>nr_topics</th>\n",
       "      <th>min_topic_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">climate</th>\n",
       "      <th rowspan=\"30\" valign=\"top\">louisbetsch/tweetclassification-bf-model</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>5</th>\n",
       "      <td>0.056019</td>\n",
       "      <td>-7.145215</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>86.014758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.803210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">20</th>\n",
       "      <th>5</th>\n",
       "      <td>0.015537</td>\n",
       "      <td>-9.109255</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>64.847164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.645188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>5</th>\n",
       "      <td>0.091241</td>\n",
       "      <td>-8.924300</td>\n",
       "      <td>0.903448</td>\n",
       "      <td>109.704391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.008008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">40</th>\n",
       "      <th>5</th>\n",
       "      <td>0.037395</td>\n",
       "      <td>-10.279334</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>176.935114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>87.406618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">50</th>\n",
       "      <th>5</th>\n",
       "      <td>0.096947</td>\n",
       "      <td>-9.233492</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>99.989024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.085581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>5</th>\n",
       "      <td>0.014814</td>\n",
       "      <td>-6.535912</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>81.884544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>59.714905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">20</th>\n",
       "      <th>5</th>\n",
       "      <td>0.102418</td>\n",
       "      <td>-7.905983</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>114.468324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81.613983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>5</th>\n",
       "      <td>0.111862</td>\n",
       "      <td>-8.417006</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>134.864426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.347457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">40</th>\n",
       "      <th>5</th>\n",
       "      <td>0.083656</td>\n",
       "      <td>-9.642550</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>156.511021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>82.892806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">50</th>\n",
       "      <th>5</th>\n",
       "      <td>0.073667</td>\n",
       "      <td>-10.083164</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>145.134579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91.199692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">10</th>\n",
       "      <th>5</th>\n",
       "      <td>0.043531</td>\n",
       "      <td>-5.978869</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>132.204142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>85.905445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">20</th>\n",
       "      <th>5</th>\n",
       "      <td>0.032168</td>\n",
       "      <td>-8.928232</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>135.231485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>66.160872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">30</th>\n",
       "      <th>5</th>\n",
       "      <td>0.096584</td>\n",
       "      <td>-8.747929</td>\n",
       "      <td>0.906897</td>\n",
       "      <td>150.196086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.741677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">40</th>\n",
       "      <th>5</th>\n",
       "      <td>0.104233</td>\n",
       "      <td>-9.161630</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>121.682531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>78.598120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">50</th>\n",
       "      <th>5</th>\n",
       "      <td>0.132470</td>\n",
       "      <td>-8.720927</td>\n",
       "      <td>0.912245</td>\n",
       "      <td>198.350233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.615377</td>\n",
       "      <td>-0.163698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>73.769040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               npmi  \\\n",
       "Dataset model                                    nr_topics min_topic_size             \n",
       "climate louisbetsch/tweetclassification-bf-model 10        5               0.056019   \n",
       "                                                           15              0.615377   \n",
       "                                                 20        5               0.015537   \n",
       "                                                           15              0.615377   \n",
       "                                                 30        5               0.091241   \n",
       "                                                           15              0.615377   \n",
       "                                                 40        5               0.037395   \n",
       "                                                           15              0.615377   \n",
       "                                                 50        5               0.096947   \n",
       "                                                           15              0.615377   \n",
       "                                                 10        5               0.014814   \n",
       "                                                           15              0.615377   \n",
       "                                                 20        5               0.102418   \n",
       "                                                           15              0.615377   \n",
       "                                                 30        5               0.111862   \n",
       "                                                           15              0.615377   \n",
       "                                                 40        5               0.083656   \n",
       "                                                           15              0.615377   \n",
       "                                                 50        5               0.073667   \n",
       "                                                           15              0.615377   \n",
       "                                                 10        5               0.043531   \n",
       "                                                           15              0.615377   \n",
       "                                                 20        5               0.032168   \n",
       "                                                           15              0.615377   \n",
       "                                                 30        5               0.096584   \n",
       "                                                           15              0.615377   \n",
       "                                                 40        5               0.104233   \n",
       "                                                           15              0.615377   \n",
       "                                                 50        5               0.132470   \n",
       "                                                           15              0.615377   \n",
       "\n",
       "                                                                               umass  \\\n",
       "Dataset model                                    nr_topics min_topic_size              \n",
       "climate louisbetsch/tweetclassification-bf-model 10        5               -7.145215   \n",
       "                                                           15              -0.163698   \n",
       "                                                 20        5               -9.109255   \n",
       "                                                           15              -0.163698   \n",
       "                                                 30        5               -8.924300   \n",
       "                                                           15              -0.163698   \n",
       "                                                 40        5              -10.279334   \n",
       "                                                           15              -0.163698   \n",
       "                                                 50        5               -9.233492   \n",
       "                                                           15              -0.163698   \n",
       "                                                 10        5               -6.535912   \n",
       "                                                           15              -0.163698   \n",
       "                                                 20        5               -7.905983   \n",
       "                                                           15              -0.163698   \n",
       "                                                 30        5               -8.417006   \n",
       "                                                           15              -0.163698   \n",
       "                                                 40        5               -9.642550   \n",
       "                                                           15              -0.163698   \n",
       "                                                 50        5              -10.083164   \n",
       "                                                           15              -0.163698   \n",
       "                                                 10        5               -5.978869   \n",
       "                                                           15              -0.163698   \n",
       "                                                 20        5               -8.928232   \n",
       "                                                           15              -0.163698   \n",
       "                                                 30        5               -8.747929   \n",
       "                                                           15              -0.163698   \n",
       "                                                 40        5               -9.161630   \n",
       "                                                           15              -0.163698   \n",
       "                                                 50        5               -8.720927   \n",
       "                                                           15              -0.163698   \n",
       "\n",
       "                                                                           diversity  \\\n",
       "Dataset model                                    nr_topics min_topic_size              \n",
       "climate louisbetsch/tweetclassification-bf-model 10        5                0.888889   \n",
       "                                                           15               1.000000   \n",
       "                                                 20        5                0.842105   \n",
       "                                                           15               1.000000   \n",
       "                                                 30        5                0.903448   \n",
       "                                                           15               1.000000   \n",
       "                                                 40        5                0.923077   \n",
       "                                                           15               1.000000   \n",
       "                                                 50        5                0.908163   \n",
       "                                                           15               1.000000   \n",
       "                                                 10        5                0.744444   \n",
       "                                                           15               1.000000   \n",
       "                                                 20        5                0.878947   \n",
       "                                                           15               1.000000   \n",
       "                                                 30        5                0.913793   \n",
       "                                                           15               1.000000   \n",
       "                                                 40        5                0.930769   \n",
       "                                                           15               1.000000   \n",
       "                                                 50        5                0.914286   \n",
       "                                                           15               1.000000   \n",
       "                                                 10        5                0.777778   \n",
       "                                                           15               1.000000   \n",
       "                                                 20        5                0.847368   \n",
       "                                                           15               1.000000   \n",
       "                                                 30        5                0.906897   \n",
       "                                                           15               1.000000   \n",
       "                                                 40        5                0.915385   \n",
       "                                                           15               1.000000   \n",
       "                                                 50        5                0.912245   \n",
       "                                                           15               1.000000   \n",
       "\n",
       "                                                                           computation_time  \n",
       "Dataset model                                    nr_topics min_topic_size                    \n",
       "climate louisbetsch/tweetclassification-bf-model 10        5                      86.014758  \n",
       "                                                           15                     39.803210  \n",
       "                                                 20        5                      64.847164  \n",
       "                                                           15                     37.645188  \n",
       "                                                 30        5                     109.704391  \n",
       "                                                           15                     50.008008  \n",
       "                                                 40        5                     176.935114  \n",
       "                                                           15                     87.406618  \n",
       "                                                 50        5                      99.989024  \n",
       "                                                           15                     56.085581  \n",
       "                                                 10        5                      81.884544  \n",
       "                                                           15                     59.714905  \n",
       "                                                 20        5                     114.468324  \n",
       "                                                           15                     81.613983  \n",
       "                                                 30        5                     134.864426  \n",
       "                                                           15                     64.347457  \n",
       "                                                 40        5                     156.511021  \n",
       "                                                           15                     82.892806  \n",
       "                                                 50        5                     145.134579  \n",
       "                                                           15                     91.199692  \n",
       "                                                 10        5                     132.204142  \n",
       "                                                           15                     85.905445  \n",
       "                                                 20        5                     135.231485  \n",
       "                                                           15                     66.160872  \n",
       "                                                 30        5                     150.196086  \n",
       "                                                           15                     62.741677  \n",
       "                                                 40        5                     121.682531  \n",
       "                                                           15                     78.598120  \n",
       "                                                 50        5                     198.350233  \n",
       "                                                           15                     73.769040  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_name = \"louisbetsch/tweetclassification-bf-model\"\n",
    "tc_results = get_bertopic_result(tc_name)\n",
    "clean_results(tc_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"all-MiniLM-L6-v2\"\n",
    "bert_results = get_bertopic_result(bert_model_name)\n",
    "clean_results(bert_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_embeddings = get_openai_embeddings(data)\n",
    "openai_results = get_bertopic_result(\"openai\", embeddings=openai_embeddings, custom=True)\n",
    "clean_results(openai_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal sentence encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 18:35:47.510801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 18:38:43.017828: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub\n",
    "embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "use_embeddings = embedding_model(data)\n",
    "use_embeddings = np.array([emb.numpy() for emb in use_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 18:45:51,501 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:45:52,731 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:45:53,934 - BERTopic - Reduced number of topics from 80 to 10\n",
      "2023-03-16 18:46:30,991 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:46:31,232 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:46:31,744 - BERTopic - Reduced number of topics from 19 to 10\n",
      "2023-03-16 18:47:01,679 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:47:01,891 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:47:02,457 - BERTopic - Reduced number of topics from 85 to 20\n",
      "2023-03-16 18:47:30,519 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:47:30,775 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:47:31,027 - BERTopic - Reduced number of topics from 18 to 18\n",
      "2023-03-16 18:47:54,290 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:47:54,570 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:47:55,858 - BERTopic - Reduced number of topics from 82 to 30\n",
      "2023-03-16 18:48:26,654 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:48:26,870 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:48:27,226 - BERTopic - Reduced number of topics from 3 to 3\n",
      "2023-03-16 18:48:52,389 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:48:52,676 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:48:53,452 - BERTopic - Reduced number of topics from 85 to 40\n",
      "2023-03-16 18:49:21,410 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:49:21,709 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:49:22,122 - BERTopic - Reduced number of topics from 17 to 17\n",
      "2023-03-16 18:49:53,532 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:49:54,074 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:49:55,382 - BERTopic - Reduced number of topics from 80 to 50\n",
      "2023-03-16 18:50:20,080 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:50:20,307 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:50:20,595 - BERTopic - Reduced number of topics from 3 to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with use 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 18:50:46,545 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:50:46,760 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:50:47,268 - BERTopic - Reduced number of topics from 81 to 10\n",
      "2023-03-16 18:51:03,297 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:51:03,465 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:51:03,651 - BERTopic - Reduced number of topics from 3 to 3\n",
      "2023-03-16 18:51:23,016 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:51:23,622 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:51:24,640 - BERTopic - Reduced number of topics from 79 to 20\n",
      "2023-03-16 18:51:49,449 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:51:49,793 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:51:49,999 - BERTopic - Reduced number of topics from 3 to 3\n",
      "2023-03-16 18:52:13,918 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:52:14,249 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:52:14,825 - BERTopic - Reduced number of topics from 74 to 30\n",
      "2023-03-16 18:52:40,374 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:52:40,635 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:52:40,894 - BERTopic - Reduced number of topics from 20 to 20\n",
      "2023-03-16 18:53:05,867 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:53:06,500 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:53:07,785 - BERTopic - Reduced number of topics from 83 to 40\n",
      "2023-03-16 18:53:31,928 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:53:32,070 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:53:32,296 - BERTopic - Reduced number of topics from 23 to 23\n",
      "2023-03-16 18:53:55,980 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:53:56,151 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:53:57,069 - BERTopic - Reduced number of topics from 79 to 50\n",
      "2023-03-16 18:54:22,000 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:54:22,356 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:54:22,592 - BERTopic - Reduced number of topics from 18 to 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with use 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 18:54:42,864 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:54:43,088 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:54:43,542 - BERTopic - Reduced number of topics from 85 to 10\n",
      "2023-03-16 18:55:01,608 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:55:01,735 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:55:01,866 - BERTopic - Reduced number of topics from 3 to 3\n",
      "2023-03-16 18:55:15,756 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:55:15,952 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:55:16,356 - BERTopic - Reduced number of topics from 76 to 20\n",
      "2023-03-16 18:55:32,788 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:55:32,917 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:55:33,037 - BERTopic - Reduced number of topics from 3 to 3\n",
      "2023-03-16 18:55:48,968 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:55:49,175 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:55:49,703 - BERTopic - Reduced number of topics from 84 to 30\n",
      "2023-03-16 18:56:04,392 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:56:04,515 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:56:04,657 - BERTopic - Reduced number of topics from 21 to 21\n",
      "2023-03-16 18:56:19,168 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:56:19,325 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:56:19,854 - BERTopic - Reduced number of topics from 86 to 40\n",
      "2023-03-16 18:56:44,854 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:56:45,018 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:56:45,195 - BERTopic - Reduced number of topics from 20 to 20\n",
      "2023-03-16 18:57:10,739 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:57:11,245 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:57:12,354 - BERTopic - Reduced number of topics from 84 to 50\n",
      "2023-03-16 18:57:37,246 - BERTopic - Reduced dimensionality\n",
      "2023-03-16 18:57:37,471 - BERTopic - Clustered reduced embeddings\n",
      "2023-03-16 18:57:37,908 - BERTopic - Reduced number of topics from 20 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with use 3\n"
     ]
    }
   ],
   "source": [
    "use_results = get_bertopic_result(\"use\", embeddings=use_embeddings, custom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"climate\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      #custom_model=Top2Vec,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=False)\n",
    "    results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 7},\n",
       "  'Scores': {'npmi': -0.09354649083222286,\n",
       "   'umass': -4.349285298199307,\n",
       "   'diversity': 0.44285714285714284},\n",
       "  'Computation Time': 21.843358993530273},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 9},\n",
       "  'Scores': {'npmi': -0.15375534843485045,\n",
       "   'umass': -6.19505904468258,\n",
       "   'diversity': 0.36666666666666664},\n",
       "  'Computation Time': 30.2948739528656},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 2},\n",
       "  'Scores': {'npmi': -0.06097515238495403,\n",
       "   'umass': -3.043228241657335,\n",
       "   'diversity': 0.75},\n",
       "  'Computation Time': 23.51417112350464},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 6},\n",
       "  'Scores': {'npmi': -0.16695670371260707,\n",
       "   'umass': -6.657668787535967,\n",
       "   'diversity': 0.5},\n",
       "  'Computation Time': 21.81808876991272},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 2},\n",
       "  'Scores': {'npmi': -0.1719080070836776,\n",
       "   'umass': -6.005185970048965,\n",
       "   'diversity': 1.0},\n",
       "  'Computation Time': 23.636770963668823}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results:\n\u001b[1;32m     10\u001b[0m         \u001b[39mfor\u001b[39;00m test \u001b[39min\u001b[39;00m result:\n\u001b[0;32m---> 11\u001b[0m                 row \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries([test[\u001b[39m'\u001b[39;49m\u001b[39mDataset\u001b[39;49m\u001b[39m'\u001b[39;49m], test[\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m], test[\u001b[39m'\u001b[39m\u001b[39mParams\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mnr_topics\u001b[39m\u001b[39m'\u001b[39m], test[\u001b[39m'\u001b[39m\u001b[39mParams\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhdbscan_args\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmin_claster_size\u001b[39m\u001b[39m'\u001b[39m], test[\u001b[39m'\u001b[39m\u001b[39mScores\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mnpmi\u001b[39m\u001b[39m'\u001b[39m], test[\u001b[39m'\u001b[39m\u001b[39mScores\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mumass\u001b[39m\u001b[39m'\u001b[39m], test[\u001b[39m'\u001b[39m\u001b[39mScores\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdiversity\u001b[39m\u001b[39m'\u001b[39m], test[\u001b[39m'\u001b[39m\u001b[39mComputation Time\u001b[39m\u001b[39m'\u001b[39m]], index \u001b[39m=\u001b[39m result_df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m     12\u001b[0m                 result_df \u001b[39m=\u001b[39m result_df\u001b[39m.\u001b[39mappend(row, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# groupby and get the mean for the 3 tests \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# create result df \n",
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "for result in results:\n",
    "        for test in result:\n",
    "                row = pd.Series([test['Dataset'], test['Model'], test['Params']['nr_topics'], test['Params']['hdbscan_args']['min_claster_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "                result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics', 'min_topic_size'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics', 'min_topic_size']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results\n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity',\n",
    "        'computation_time']].to_csv('bertopic'+'openai'+'.csv', index=False)\n",
    "        \n",
    "return result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42353f8e58af4176f2e91cb75b1ead06ad78851c830c26089e6dc288583b73d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
