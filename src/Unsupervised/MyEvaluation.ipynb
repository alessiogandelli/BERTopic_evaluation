{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will evaluate some topic modeling techniques\n",
    "- LDA\n",
    "- NMF \n",
    "- Top2Vec\n",
    "- Bertopic \n",
    "    - bert classic (miniml-6)\n",
    "    - openai\n",
    "    - tweet-classification\n",
    "    - climabert\n",
    "    - universal sentence encoder\n",
    "\n",
    "\n",
    "for each of this i made different test with different parameters and different datasets changing the numebr of topics\n",
    "\n",
    "Dataset:\n",
    "- climate: 1669 preprocessed tweets \n",
    "- todo other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import Trainer, DataLoader\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=\"climate\").prepare_docs(save=\"climate.txt\").preprocess_octis(output_folder=\"climate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the next cell we train the model multiple times, first changing the parameters of num_topics (10 to 50 with step 10) and all this 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset = \"climate\"\n",
    "    custom = True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "                      \n",
    "    results = trainer.train(save=f\"NMF_climate_{i+1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save all the results in a dataframe, we compute the average on the 3 runs and save the results in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "\n",
    "for test in results:\n",
    "        row = pd.Series([test['Dataset'], test['Model'], test['Params']['num_topics'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "        result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results in a file \n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'npmi', 'umass', 'diversity', 'computation_time']].to_csv('bertopic'+'nmf'+'.csv', index=False)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"climate\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "\n",
    "for test in results:\n",
    "        row = pd.Series([test['Dataset'], test['Model'], test['Params']['num_topics'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "        result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results\n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'npmi', 'umass', 'diversity',\n",
    "        'computation_time']].to_csv('LDA'+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertopic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data preparation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we prepare the data for the bertopic model, so we get back the data as a list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Prepare data\n",
    "dataset, custom = \"climate\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a sentence_transformers model name or directly embeddings to evaluate \n",
    "def get_bertopic_result(model_name, embeddings = None, custom = False):\n",
    "\n",
    "        # get emebddings if not provided\n",
    "        display_name = model_name\n",
    "        model_name = None\n",
    "        results = []\n",
    "\n",
    "        if not custom:\n",
    "                model = SentenceTransformer(display_name)\n",
    "                embeddings = model.encode(data, show_progress_bar=True)\n",
    "                print('embedded')\n",
    "                model_name = display_name\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "        # do this 3 times\n",
    "        for i in range(3):\n",
    "                # params that will be passed to Bertopic, model name none for custom embeddings\n",
    "                params = {\n",
    "                        \"embedding_model\": model_name,\n",
    "                        \"nr_topics\": [(i+1)*10 for i in range(5)],  # 10, 20, 30, 40, 50 topics\n",
    "                        \"min_topic_size\": [5,15],                   # 5, 15 documents per topic\n",
    "                        \"verbose\": True\n",
    "                }\n",
    "                # train\n",
    "                trainer = Trainer(      dataset=dataset,\n",
    "                                        model_name=\"BERTopic\",\n",
    "                                        params=params,\n",
    "                                        bt_embeddings=embeddings,\n",
    "                                        custom_dataset=True,\n",
    "                                        verbose=False)\n",
    "\n",
    "                results.append(trainer.train())\n",
    "                print(f\"Done with {display_name} {i+1}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# given the results \n",
    "def clean_results(results):\n",
    "        # create result df \n",
    "        result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "        # dataset used and model \n",
    "        dataset = results[0][0]['Dataset']\n",
    "        model = results[0][0]['Params']['embedding_model']\n",
    "\n",
    "        # fill the dataframe \n",
    "        for result in results:\n",
    "                for test in result:\n",
    "                        pd.Series([test['Dataset'], test['Params']['embedding_model'], test['Params']['nr_topics'], test['Params']['min_topic_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "                        result_df = result_df.append(pd.Series([test['Dataset'], test['Params']['embedding_model'], test['Params']['nr_topics'], test['Params']['min_topic_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns), ignore_index=True)\n",
    "\n",
    "        # groupby and get the mean for the 3 tests \n",
    "        result_df.set_index(['Dataset', 'model', 'nr_topics', 'min_topic_size'], inplace=True)\n",
    "        a = result_df.groupby(['nr_topics', 'min_topic_size']).mean()\n",
    "        a.reset_index(inplace=True)\n",
    "\n",
    "        #save the results\n",
    "        a['dataset'] = dataset\n",
    "        a['model'] = model\n",
    "        a[['dataset', 'model','nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity',\n",
    "                'computation_time']].to_csv('bertopic'+'climabert'+'.csv', index=False)\n",
    "                \n",
    "        return result_df\n",
    "\n",
    "def get_openai_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "        texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "\n",
    "        embs = openai.Embedding.create(input = texts, model=model)['data']\n",
    "        return np.array([np.array(emb['embedding']) for emb in embs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### climatebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatebert_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "climatebert_results = get_bertopic_result(climatebert_name)\n",
    "clean_results(climatebert_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweetclassifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_name = \"louisbetsch/tweetclassification-bf-model\"\n",
    "tc_results = get_bertopic_result(tc_name)\n",
    "clean_results(tc_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"all-MiniLM-L6-v2\"\n",
    "bert_results = get_bertopic_result(bert_model_name)\n",
    "clean_results(bert_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_embeddings = get_openai_embeddings(data)\n",
    "openai_results = get_bertopic_result(\"openai\", embeddings=openai_embeddings, custom=True)\n",
    "clean_results(openai_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal sentence encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub\n",
    "embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "use_embeddings = embedding_model(data)\n",
    "use_embeddings = np.array([emb.numpy() for emb in use_embeddings])\n",
    "use_results = get_bertopic_result(\"use\", embeddings=use_embeddings, custom=True)\n",
    "clean_results(use_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"climate\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      #custom_model=Top2Vec,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=False)\n",
    "    results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result df \n",
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "\n",
    "for test in results:\n",
    "        row = pd.Series([test['Dataset'], test['Model'], test['Params']['nr_topics'], test['Params']['hdbscan_args']['min_cluster_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "        result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics', 'min_topic_size'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics', 'min_topic_size']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results\n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity',\n",
    "        'computation_time']].to_csv('bertopic'+'openai'+'.csv', index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42353f8e58af4176f2e91cb75b1ead06ad78851c830c26089e6dc288583b73d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
