{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will evaluate some topic modeling techniques\n",
    "- LDA\n",
    "- NMF \n",
    "- Top2Vec\n",
    "- Bertopic \n",
    "    - bert classic (miniml-6)\n",
    "    - openai\n",
    "    - tweet-classification\n",
    "    - climabert\n",
    "    - universal sentence encoder\n",
    "\n",
    "\n",
    "for each of this i made different test with different parameters and different datasets changing the numebr of topics\n",
    "\n",
    "Dataset:\n",
    "- climate: 1669 preprocessed tweets \n",
    "- todo other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 16:14:30.055781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alessiogandelli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import Trainer, DataLoader\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading climate data\n",
      "created vocab\n",
      "5225\n",
      "words filtering done\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=\"climate\").prepare_docs(save=\"climate.txt\").preprocess_octis(output_folder=\"climate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the next cell we train the model multiple times, first changing the parameters of num_topics (10 to 50 with step 10) and all this 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset = \"climate\"\n",
    "    custom = True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"NMF\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "                      \n",
    "    results = trainer.train(save=f\"NMF_climate_{i+1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save all the results in a dataframe, we compute the average on the 3 runs and save the results in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "\n",
    "for test in results:\n",
    "        row = pd.Series([test['Dataset'], test['Model'], test['Params']['num_topics'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "        result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results in a file \n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'npmi', 'umass', 'diversity', 'computation_time']].to_csv('bertopic'+'nmf'+'.csv', index=False)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, random_state in enumerate([0, 21, 42]):\n",
    "    dataset, custom = \"climate\", True\n",
    "    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "\n",
    "for test in results:\n",
    "        row = pd.Series([test['Dataset'], test['Model'], test['Params']['num_topics'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "        result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results\n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'npmi', 'umass', 'diversity',\n",
    "        'computation_time']].to_csv('LDA'+'.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertopic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data preparation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we prepare the data for the bertopic model, so we get back the data as a list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Prepare data\n",
    "dataset, custom = \"climate\", True\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a sentence_transformers model name or directly embeddings to evaluate \n",
    "def get_bertopic_result(model_name, embeddings = None, custom = False):\n",
    "\n",
    "        # get emebddings if not provided\n",
    "        display_name = model_name\n",
    "        model_name = None\n",
    "        results = []\n",
    "\n",
    "        if not custom:\n",
    "                model = SentenceTransformer(display_name)\n",
    "                embeddings = model.encode(data, show_progress_bar=True)\n",
    "                print('embedded')\n",
    "                model_name = display_name\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "        # do this 3 times\n",
    "        for i in range(3):\n",
    "                # params that will be passed to Bertopic, model name none for custom embeddings\n",
    "                params = {\n",
    "                        \"embedding_model\": model_name,\n",
    "                        \"nr_topics\": [(i+1)*10 for i in range(5)],  # 10, 20, 30, 40, 50 topics\n",
    "                        \"min_topic_size\": [5,15],                   # 5, 15 documents per topic\n",
    "                        \"verbose\": True\n",
    "                }\n",
    "                # train\n",
    "                trainer = Trainer(      dataset=dataset,\n",
    "                                        model_name=\"BERTopic\",\n",
    "                                        params=params,\n",
    "                                        bt_embeddings=embeddings,\n",
    "                                        custom_dataset=True,\n",
    "                                        verbose=False)\n",
    "\n",
    "                results.append(trainer.train())\n",
    "                print(f\"Done with {display_name} {i+1}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# given the results \n",
    "def clean_results(results):\n",
    "        # create result df \n",
    "        result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "        # dataset used and model \n",
    "        dataset = results[0][0]['Dataset']\n",
    "        model = results[0][0]['Params']['embedding_model']\n",
    "\n",
    "        # fill the dataframe \n",
    "        for result in results:\n",
    "                for test in result:\n",
    "                        pd.Series([test['Dataset'], test['Params']['embedding_model'], test['Params']['nr_topics'], test['Params']['min_topic_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "                        result_df = result_df.append(pd.Series([test['Dataset'], test['Params']['embedding_model'], test['Params']['nr_topics'], test['Params']['min_topic_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns), ignore_index=True)\n",
    "\n",
    "        # groupby and get the mean for the 3 tests \n",
    "        result_df.set_index(['Dataset', 'model', 'nr_topics', 'min_topic_size'], inplace=True)\n",
    "        a = result_df.groupby(['nr_topics', 'min_topic_size']).mean()\n",
    "        a.reset_index(inplace=True)\n",
    "\n",
    "        #save the results\n",
    "        a['dataset'] = dataset\n",
    "        a['model'] = model\n",
    "        a[['dataset', 'model','nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity',\n",
    "                'computation_time']].to_csv('bertopic'+'climabert'+'.csv', index=False)\n",
    "                \n",
    "        return result_df\n",
    "\n",
    "def get_openai_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "        texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "\n",
    "        embs = openai.Embedding.create(input = texts, model=model)['data']\n",
    "        return np.array([np.array(emb['embedding']) for emb in embs])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### climatebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatebert_name = \"climatebert/distilroberta-base-climate-f\"\n",
    "climatebert_results = get_bertopic_result(climatebert_name)\n",
    "clean_results(climatebert_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweetclassifcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_name = \"louisbetsch/tweetclassification-bf-model\"\n",
    "tc_results = get_bertopic_result(tc_name)\n",
    "clean_results(tc_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"all-MiniLM-L6-v2\"\n",
    "bert_results = get_bertopic_result(bert_model_name)\n",
    "clean_results(bert_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_embeddings = get_openai_embeddings(data)\n",
    "openai_results = get_bertopic_result(\"openai\", embeddings=openai_embeddings, custom=True)\n",
    "clean_results(openai_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal sentence encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub\n",
    "embedding_model = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "use_embeddings = embedding_model(data)\n",
    "use_embeddings = np.array([emb.numpy() for emb in use_embeddings])\n",
    "use_results = get_bertopic_result(\"use\", embeddings=use_embeddings, custom=True)\n",
    "clean_results(use_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 14:11:41,686 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:11:42,360 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:11:52,311 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:12:15,671 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:12:16,123 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:12:17,325 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:12:17,680 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:12:26,041 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:13:02,695 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:13:03,068 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:13:03,798 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:13:04,099 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:13:16,948 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:13:47,343 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:13:47,958 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:13:50,815 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:13:52,333 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:14:08,862 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:15:20,106 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:15:20,943 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:15:24,205 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:15:24,813 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:15:39,076 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:16:25,345 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:16:26,300 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:16:28,935 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:16:29,441 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:16:43,919 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:17:45,398 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:17:46,417 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:17:49,745 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:17:50,295 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:18:03,902 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:18:51,553 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:18:52,029 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:18:54,294 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:18:55,349 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:19:10,340 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:20:00,528 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:20:01,244 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:20:03,443 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:20:04,165 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:20:18,903 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:20:59,468 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:21:00,356 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:21:02,545 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:21:03,192 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:21:20,053 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:21:54,966 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:21:55,912 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:21:59,648 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:22:00,403 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:22:12,702 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:22:56,452 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:22:57,178 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:22:59,281 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:22:59,827 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:23:13,438 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:23:50,415 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:23:50,884 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:23:52,482 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:23:53,002 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:24:05,150 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:24:53,869 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:24:54,508 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:24:56,781 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:24:57,342 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:25:19,385 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:25:57,316 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:25:58,085 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n",
      "2023-03-17 14:26:02,182 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "2023-03-17 14:26:03,208 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-17 14:26:16,079 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-17 14:26:52,558 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-17 14:26:53,347 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dataset, custom = \"climate\", True\n",
    "    params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "              # \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "              \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                               'metric': 'euclidean',\n",
    "                               'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      custom_dataset=custom,\n",
    "                      #custom_model=Top2Vec,\n",
    "                      model_name=\"Top2Vec\",\n",
    "                      params=params,\n",
    "                      verbose=False)\n",
    "    results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 7},\n",
       "  'Scores': {'npmi': -0.1587504863442093,\n",
       "   'umass': -6.289935931655772,\n",
       "   'diversity': 0.44285714285714284},\n",
       "  'Computation Time': 57.5817129611969},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 2},\n",
       "  'Scores': {'npmi': -0.14258214992671217,\n",
       "   'umass': -5.516094819047652,\n",
       "   'diversity': 0.9},\n",
       "  'Computation Time': 51.67424297332764},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 6},\n",
       "  'Scores': {'npmi': -0.11884787831816628,\n",
       "   'umass': -5.337466735878532,\n",
       "   'diversity': 0.4666666666666667},\n",
       "  'Computation Time': 62.3788697719574},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 4},\n",
       "  'Scores': {'npmi': -0.0857398839727988,\n",
       "   'umass': -4.235980235720835,\n",
       "   'diversity': 0.6},\n",
       "  'Computation Time': 61.540796995162964},\n",
       " {'Dataset': 'climate',\n",
       "  'Dataset Size': 1669,\n",
       "  'Model': 'Top2Vec',\n",
       "  'Params': {'hdbscan_args': {'min_cluster_size': 15,\n",
       "    'metric': 'euclidean',\n",
       "    'cluster_selection_method': 'eom'},\n",
       "   'reduction': False,\n",
       "   'nr_topics': 7},\n",
       "  'Scores': {'npmi': -0.12704382550525803,\n",
       "   'umass': -5.366913558302946,\n",
       "   'diversity': 0.45714285714285713},\n",
       "  'Computation Time': 51.31413817405701}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create result df \n",
    "result_df = pd.DataFrame( columns = ['Dataset', 'model', 'nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity', 'computation_time'])\n",
    "\n",
    "# dataset used and model \n",
    "dataset = results[0]['Dataset']\n",
    "model = results[0]['Model']\n",
    "\n",
    "# fill the dataframe \n",
    "\n",
    "for test in results:\n",
    "        row = pd.Series([test['Dataset'], test['Model'], test['Params']['nr_topics'], test['Params']['hdbscan_args']['min_cluster_size'], test['Scores']['npmi'], test['Scores']['umass'], test['Scores']['diversity'], test['Computation Time']], index = result_df.columns)\n",
    "        result_df = result_df.append(row, ignore_index=True)\n",
    "\n",
    "# groupby and get the mean for the 3 tests \n",
    "result_df.set_index(['Dataset', 'model', 'nr_topics', 'min_topic_size'], inplace=True)\n",
    "a = result_df.groupby(['nr_topics', 'min_topic_size']).mean()\n",
    "a.reset_index(inplace=True)\n",
    "\n",
    "#save the results\n",
    "a['dataset'] = dataset\n",
    "a['model'] = model\n",
    "a[['dataset', 'model','nr_topics', 'min_topic_size', 'npmi', 'umass', 'diversity',\n",
    "        'computation_time']].to_csv('bertopic'+'openai'+'.csv', index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42353f8e58af4176f2e91cb75b1ead06ad78851c830c26089e6dc288583b73d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
